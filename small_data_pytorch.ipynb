{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative and DL techniques on small metabolic engineering datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on cuda\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ERG8</th>\n",
       "      <th>ERG10</th>\n",
       "      <th>ERG12</th>\n",
       "      <th>ERG13</th>\n",
       "      <th>ERG19</th>\n",
       "      <th>ERG20</th>\n",
       "      <th>IDI1</th>\n",
       "      <th>tHMG1</th>\n",
       "      <th>prod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.862946</td>\n",
       "      <td>0.987876</td>\n",
       "      <td>0.892209</td>\n",
       "      <td>0.909210</td>\n",
       "      <td>0.848096</td>\n",
       "      <td>0.840536</td>\n",
       "      <td>0.829163</td>\n",
       "      <td>0.753874</td>\n",
       "      <td>3.739360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.783979</td>\n",
       "      <td>1.001960</td>\n",
       "      <td>0.978865</td>\n",
       "      <td>0.916151</td>\n",
       "      <td>0.718987</td>\n",
       "      <td>0.827099</td>\n",
       "      <td>0.960778</td>\n",
       "      <td>0.770533</td>\n",
       "      <td>2.448719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.870862</td>\n",
       "      <td>0.796111</td>\n",
       "      <td>1.096750</td>\n",
       "      <td>0.919447</td>\n",
       "      <td>0.770546</td>\n",
       "      <td>0.879257</td>\n",
       "      <td>0.956943</td>\n",
       "      <td>0.909107</td>\n",
       "      <td>2.454255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ERG8     ERG10     ERG12     ERG13     ERG19     ERG20      IDI1  \\\n",
       "0  0.862946  0.987876  0.892209  0.909210  0.848096  0.840536  0.829163   \n",
       "1  0.783979  1.001960  0.978865  0.916151  0.718987  0.827099  0.960778   \n",
       "2  0.870862  0.796111  1.096750  0.919447  0.770546  0.879257  0.956943   \n",
       "\n",
       "      tHMG1      prod  \n",
       "0  0.753874  3.739360  \n",
       "1  0.770533  2.448719  \n",
       "2  0.909107  2.454255  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Working on {device}')\n",
    "\n",
    "DATA_PATH = './data/carotenoid_production.csv'\n",
    "\n",
    "df_base = pd.read_csv('./data/carotenoid_production.csv')\n",
    "cols = df_base.columns\n",
    "df_base.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def load_and_standardize_data(path=DATA_PATH):\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.values.reshape(-1, df.shape[1]).astype('float32')\n",
    "    X_train, X_test = train_test_split(df, test_size=0.3, random_state=42)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)   \n",
    "    return X_train, X_test, scaler\n",
    "\n",
    "class DataBuilder(Dataset):\n",
    "    def __init__(self, path, train=True):\n",
    "        self.X_train, self.X_test, self.standardizer = load_and_standardize_data(DATA_PATH)\n",
    "        if train:\n",
    "            self.x = torch.from_numpy(self.X_train)\n",
    "            self.len=self.x.shape[0]\n",
    "        else:\n",
    "            self.x = torch.from_numpy(self.X_test)\n",
    "            self.len=self.x.shape[0]\n",
    "        del self.X_train\n",
    "        del self.X_test \n",
    "    def __getitem__(self,index):      \n",
    "        return self.x[index]\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata_set=DataBuilder(DATA_PATH, train=True)\n",
    "testdata_set=DataBuilder(DATA_PATH, train=False)\n",
    "\n",
    "trainloader=DataLoader(dataset=traindata_set,batch_size=1024)\n",
    "testloader=DataLoader(dataset=testdata_set,batch_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VAE for generating synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, D_in, H=50, H2=12, latent_dim=3):\n",
    "        \n",
    "        # Encoder\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.linear1 = nn.Linear(D_in, H)\n",
    "        self.lin_bn1 = nn.BatchNorm1d(num_features=H)\n",
    "        self.linear2 = nn.Linear(H, H2)\n",
    "        self.lin_bn2 = nn.BatchNorm1d(num_features=H2)\n",
    "        self.linear3 = nn.Linear(H2, H2)\n",
    "        self.lin_bn3 = nn.BatchNorm1d(num_features=H2)\n",
    "        \n",
    "#         # Latent vectors mu and sigma\n",
    "        self.fc1 = nn.Linear(H2, latent_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=latent_dim)\n",
    "        self.fc21 = nn.Linear(latent_dim, latent_dim)\n",
    "        self.fc22 = nn.Linear(latent_dim, latent_dim)\n",
    "\n",
    "#         # Sampling vector\n",
    "        self.fc3 = nn.Linear(latent_dim, latent_dim)\n",
    "        self.fc_bn3 = nn.BatchNorm1d(latent_dim)\n",
    "        self.fc4 = nn.Linear(latent_dim, H2)\n",
    "        self.fc_bn4 = nn.BatchNorm1d(H2)\n",
    "        \n",
    "#         # Decoder\n",
    "        self.linear4=nn.Linear(H2,H2)\n",
    "        self.lin_bn4 = nn.BatchNorm1d(num_features=H2)\n",
    "        self.linear5=nn.Linear(H2,H)\n",
    "        self.lin_bn5 = nn.BatchNorm1d(num_features=H)\n",
    "        self.linear6=nn.Linear(H,D_in)\n",
    "        self.lin_bn6 = nn.BatchNorm1d(num_features=D_in)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def encode(self, x):\n",
    "        lin1 = self.relu(self.lin_bn1(self.linear1(x)))\n",
    "        lin2 = self.relu(self.lin_bn2(self.linear2(lin1)))\n",
    "        lin3 = self.relu(self.lin_bn3(self.linear3(lin2)))\n",
    "\n",
    "        fc1 = F.relu(self.bn1(self.fc1(lin3)))\n",
    "\n",
    "        r1 = self.fc21(fc1)\n",
    "        r2 = self.fc22(fc1)\n",
    "        \n",
    "        return r1, r2\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = Variable(std.data.new(std.size()).normal_())\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "        \n",
    "    def decode(self, z):\n",
    "        fc3 = self.relu(self.fc_bn3(self.fc3(z)))\n",
    "        fc4 = self.relu(self.fc_bn4(self.fc4(fc3)))\n",
    "\n",
    "        lin4 = self.relu(self.lin_bn4(self.linear4(fc4)))\n",
    "        lin5 = self.relu(self.lin_bn5(self.linear5(lin4)))\n",
    "        return self.lin_bn6(self.linear6(lin5))\n",
    "\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        # self.decode(z) ist spÃ¤ter recon_batch, mu ist mu und logvar ist logvar\n",
    "        return self.decode(z), mu, logvar  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(customLoss, self).__init__()\n",
    "        self.mse_loss = nn.MSELoss(reduction=\"sum\")\n",
    "    \n",
    "    # x_recon ist der im forward im Model erstellte recon_batch, x ist der originale x Batch, mu ist mu und logvar ist logvar \n",
    "    def forward(self, x_recon, x, mu, logvar):\n",
    "        loss_MSE = self.mse_loss(x_recon, x)\n",
    "        loss_KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "        return loss_MSE + loss_KLD\n",
    "    \n",
    "# takes in a module and applies the specified weight initialization\n",
    "def weights_init_uniform_rule(m):\n",
    "    classname = m.__class__.__name__\n",
    "    # for every Linear layer in a model..\n",
    "    if classname.find('Linear') != -1:\n",
    "        # get the number of the inputs\n",
    "        n = m.in_features\n",
    "        y = 1.0/np.sqrt(n)\n",
    "        m.weight.data.uniform_(-y, y)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_in = traindata_set.x.shape[1]\n",
    "H = 50\n",
    "H2 = 12\n",
    "model = Autoencoder(D_in, H, H2).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_mse = customLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1500\n",
    "log_interval = 50\n",
    "val_losses = []\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in enumerate(trainloader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_mse(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    if epoch % 200 == 0:        \n",
    "        print('====> Epoch: {} Average training loss: {:.4f}'.format(\n",
    "            epoch, train_loss / len(trainloader.dataset)))\n",
    "        train_losses.append(train_loss / len(trainloader.dataset))\n",
    "        \n",
    "def test(epoch):\n",
    "    with torch.no_grad():\n",
    "        test_loss = 0\n",
    "        for batch_idx, data in enumerate(testloader):\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            loss = loss_mse(recon_batch, data, mu, logvar)\n",
    "            test_loss += loss.item()\n",
    "            if epoch % 200 == 0:        \n",
    "                print('====> Epoch: {} Average test loss: {:.4f}'.format(\n",
    "                    epoch, test_loss / len(testloader.dataset)))\n",
    "            test_losses.append(test_loss / len(testloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 200 Average training loss: 10.3517\n",
      "====> Epoch: 200 Average test loss: 11.6422\n",
      "====> Epoch: 400 Average training loss: 8.1159\n",
      "====> Epoch: 400 Average test loss: 9.2272\n",
      "====> Epoch: 600 Average training loss: 6.6655\n",
      "====> Epoch: 600 Average test loss: 8.4034\n",
      "====> Epoch: 800 Average training loss: 6.3386\n",
      "====> Epoch: 800 Average test loss: 8.6102\n",
      "====> Epoch: 1000 Average training loss: 5.7577\n",
      "====> Epoch: 1000 Average test loss: 8.9381\n",
      "====> Epoch: 1200 Average training loss: 5.7880\n",
      "====> Epoch: 1200 Average test loss: 8.4799\n",
      "====> Epoch: 1400 Average training loss: 5.3308\n",
      "====> Epoch: 1400 Average test loss: 8.3497\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for batch_idx, data in enumerate(testloader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use of SDV library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/WUR/g0012069/env/main/lib/python3.10/site-packages/sdv/single_table/base.py:82: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sdv.single_table import TVAESynthesizer\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "\n",
    "df = pd.read_csv('./data/carotenoid_production.csv')\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.1)\n",
    "\n",
    "metadata = SingleTableMetadata()\n",
    "metadata.detect_from_dataframe(train)\n",
    "\n",
    "synthesizer = TVAESynthesizer(metadata=metadata)\n",
    "synthesizer.fit(train)                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic data shape: (251, 9)\n"
     ]
    }
   ],
   "source": [
    "synthetic_data = synthesizer.sample(num_rows=100)\n",
    "synthetic_data = pd.concat([train, synthetic_data])\n",
    "print(f'Synthetic data shape: {synthetic_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop('prod', axis=1).copy()\n",
    "y_train = train['prod'].copy()\n",
    "\n",
    "X_test = test.drop('prod', axis=1).copy()\n",
    "y_test = test['prod'].copy()\n",
    "\n",
    "X_synthetic = synthetic_data.drop('prod', axis=1).copy()\n",
    "y_synthetic = synthetic_data['prod'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 2.6883\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': randint(10, 100),\n",
    "    'max_depth': randint(1, 10),\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "search = RandomizedSearchCV(rf, param_distributions=param_dist, n_iter=100, cv=5, random_state=42)\n",
    "search.fit(X_train, y_train)\n",
    "best_rf = search.best_estimator_\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred = best_rf.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f'Mean Squared Error: {mse:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 2.1662\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(random_state=42)\n",
    "search = RandomizedSearchCV(rf, param_distributions=param_dist, n_iter=100, cv=5, random_state=42)\n",
    "search.fit(X_synthetic, y_synthetic)\n",
    "best_rf = search.best_estimator_\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred = best_rf.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f'Mean Squared Error: {mse:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
